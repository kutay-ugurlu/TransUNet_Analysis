@article{chen2021transunet,
  title={Transunet: Transformers make strong encoders for medical image segmentation},
  author={Chen, Jieneng and Lu, Yongyi and Yu, Qihang and Luo, Xiangde and Adeli, Ehsan and Wang, Yan and Lu, Le and Yuille, Alan L and Zhou, Yuyin},
  journal={arXiv preprint arXiv:2102.04306},
  year={2021}
}

@inproceedings{gonzalez2020ecgi,
  title={ECGi Metrics in Atrial Fibrillation Dependency on Epicardium Segmentation},
  author={Gonz{\'a}lez-Ascaso, Ana and Molero, Rub{\'e}n and Climent, Andreu M and Guillem, Mar{\'\i}a S},
  booktitle={2020 Computing in Cardiology},
  pages={1--4},
  year={2020},
  organization={IEEE}
}

@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={International Conference on Medical image computing and computer-assisted intervention},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{ren2019brain,
  title={Brain MR image segmentation in small dataset with adversarial defense and task reorganization},
  author={Ren, Xuhua and Zhang, Lichi and Wei, Dongming and Shen, Dinggang and Wang, Qian},
  booktitle={International Workshop on Machine Learning in Medical Imaging},
  pages={1--8},
  year={2019},
  organization={Springer}
}

@inproceedings{de2015deep,
  title={Deep neural networks for anatomical brain segmentation},
  author={de Brebisson, Alexander and Montana, Giovanni},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition workshops},
  pages={20--28},
  year={2015}
}

@article{ZHUANG2019101537,
title = {Evaluation of algorithms for Multi-Modality Whole Heart Segmentation: An open-access grand challenge},
journal = {Medical Image Analysis},
volume = {58},
pages = {101537},
year = {2019},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2019.101537},
url = {https://www.sciencedirect.com/science/article/pii/S1361841519300751},
author = {Xiahai Zhuang and Lei Li and Christian Payer and Darko Štern and Martin Urschler and Mattias P. Heinrich and Julien Oster and Chunliang Wang and Örjan Smedby and Cheng Bian and Xin Yang and Pheng-Ann Heng and Aliasghar Mortazi and Ulas Bagci and Guanyu Yang and Chenchen Sun and Gaetan Galisot and Jean-Yves Ramel and Thierry Brouard and Qianqian Tong and Weixin Si and Xiangyun Liao and Guodong Zeng and Zenglin Shi and Guoyan Zheng and Chengjia Wang and Tom MacGillivray and David Newby and Kawal Rhode and Sebastien Ourselin and Raad Mohiaddin and Jennifer Keegan and David Firmin and Guang Yang},
keywords = {Whole Heart Segmentation, Multi-modality, Benchmark, Challenge},
abstract = {Knowledge of whole heart anatomy is a prerequisite for many clinical applications. Whole heart segmentation (WHS), which delineates substructures of the heart, can be very valuable for modeling and analysis of the anatomy and functions of the heart. However, automating this segmentation can be challenging due to the large variation of the heart shape, and different image qualities of the clinical data. To achieve this goal, an initial set of training data is generally needed for constructing priors or for training. Furthermore, it is difficult to perform comparisons between different methods, largely due to differences in the datasets and evaluation metrics used. This manuscript presents the methodologies and evaluation results for the WHS algorithms selected from the submissions to the Multi-Modality Whole Heart Segmentation (MM-WHS) challenge, in conjunction with MICCAI 2017. The challenge provided 120 three-dimensional cardiac images covering the whole heart, including 60 CT and 60 MRI volumes, all acquired in clinical environments with manual delineation. Ten algorithms for CT data and eleven algorithms for MRI data, submitted from twelve groups, have been evaluated. The results showed that the performance of CT WHS was generally better than that of MRI WHS. The segmentation of the substructures for different categories of patients could present different levels of challenge due to the difference in imaging and variations of heart shapes. The deep learning (DL)-based methods demonstrated great potential, though several of them reported poor results in the blinded evaluation. Their performance could vary greatly across different network structures and training strategies. The conventional algorithms, mainly based on multi-atlas segmentation, demonstrated good performance, though the accuracy and computational efficiency could be limited. The challenge, including provision of the annotated training data and the blinded evaluation for submitted algorithms on the test data, continues as an ongoing benchmarking resource via its homepage (www.sdspeople.fudan.edu.cn/zhuangxiahai/0/mmwhs/).}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{azad2022contextual,
  title={Contextual attention network: Transformer meets u-net},
  author={Azad, Reza and Heidari, Moein and Wu, Yuli and Merhof, Dorit},
  booktitle={Machine Learning in Medical Imaging: 13th International Workshop, MLMI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 18, 2022, Proceedings},
  pages={377--386},
  year={2022},
  organization={Springer}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

 @misc{doshi_2021, title={Transformers explained visually (part 3): Multi-head attention, Deep Dive}, url={https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853}, journal={Medium}, publisher={Towards Data Science}, author={Doshi, Ketan}, year={2021}, month={Jun}} 